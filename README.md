# LLM-Based-Judging-Architectures

<p align="center">
  <img src="images/architecture.png" alt="Architectures Diagram" width="90%" />
</p>

## Background
As large language models (LLMs) evolve, evaluating their outputs becomes increasingly complex. Traditional methods, such as human assessments, are often costly and inconsistent, while automated metrics may fail to capture the nuances of LLM performance.

To address these challenges, we propose a novel framework that uses LLMs as **advocates** in a dynamic, **courtroom-inspired** system. This approach involves LLMs acting as advocates, judges, and juries to provide a comprehensive evaluation of model outputs.

This framework integrates decision theory, bounded rationality, and economic incentives to offer a robust evaluation method. Subsequent sections will detail the architecture and experiments validating its effectiveness.



## Data

### Dataset

This project uses the `lmsys/mt_bench_human_judgments` dataset from Hugging Face.

### Preprocessing

The script `src/preprocess_mt_bench.py` processes the raw data into an Excel file (`data/mt_bench_human_judgments.xlsx`) with the following columns:

- **Question**: Aggregated user questions.
- **Response_A**: Responses from Model A.
- **Response_B**: Responses from Model B.
- **Model_A_Score**: Binary score for Model A (1 for a win, 0 for a loss).
- **Model_B_Score**: Binary score for Model B (1 for a win, 0 for a loss).

Run the preprocessing with:
```bash
python data/preprocess_mt_bench.py
```
## Installation and Setup

Our agentic architecture is built on **MetaGPT**, a framework designed to efficiently manage interactions between agents using shared memory.

To use the MetaGPT framework, follow these steps:

1. **Clone the Repository**
   
First, clone the repository and navigate to the relevant directory:
```bash
git clone https://github.com/abirharrasse/LLM-Judging-Architectures  && cd LLM-Judging-Architectures/MetaGPT_LLM_advocates
```
3. **Install Dependencies**
   
Install the necessary packages:
```bash
pip install --upgrade -e .
pip install together -q
```
3. Initialize Configuration

Set up the configuration file for the MetaGPT framework:
```bash
metagpt --init-config
```
Before running this command, ensure you're in the correct directory:
```bash
import os
os.chdir('/content/LLM-Judging-Architectures/MetaGPT_LLM_advocates')
print(os.getcwd())
```
4. Set API Keys
   
Set the required API keys to run your experiments:
```bash
os.environ['OPENAI_API_KEY'] = 'sk-'
os.environ['TOGETHER_API_KEY'] = '
os.environ['CLAUDE_API_KEY'] = ''
os.environ['GEMINI_API_KEY'] = ''
os.environ['COHERE_API_KEY'] = ''
```
## Running the evaluation architectures

#### Single Advocate Multi-Round Evaluation (SAMRE): 
To run the **SAMRE** evaluation framework:
Begin by importing the required function:
```bash
from experiments import samre_experiment
```
then call the `samre_experiment` function with the appropriate parameters: 
```bash
samre_experiment(model, temperature, question, answer1, answer2, investment=0.1, n_rounds=4, n_juries=3)
```
Where:

- **model**: Select one of the models supported by our framework, accessible via Together or OpenAI.
- **question**: The question to which answer1 and answer2 respond.
- **answer1** and **answer2**: The answers to be evaluated.
- **investment**: The maximum cost allocated for the experiment.
- **n_rounds**: The number of evaluation rounds to conduct.
- **n_juries**: The number of juries involved in the evaluation.



#### Multi-Advocate One-Round Evaluation (MORE) 

To run the **MORE** evaluation framework:
Begin by importing:
```bash
from experiments import more_experiment
```
then call the `more_experiment` function with the appropriate parameters: 
```bash
more_experiment(model_pl, temperature, question, answer1, answer2, n_advocates=3, investment=0.1, n_rounds=1)
```
Where:
- **n_advocates**: The number of advocates involved in the evaluation.
- **n_rounds**: The number of evaluation rounds to conduct.


## Full Dataset Evaluation

To execute our architectures on the entire `mt-bench` dataset and ompare it to the basemodel, use the following code:
```bash
!python utils/main.py
```
